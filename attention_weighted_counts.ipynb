{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# All- The-News Data Set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This notebook is to import the all-the-news data set (downloaded from https://tinyurl.com/bx3r3de8) as in Rochelle's stepmothers paper. Next step will be to fine tune BERT on the different news sources. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Quote from paper: 'From each\n",
    "news source we take 4354 articles from the All- The-News3 dataset that contains articles from 27 American Publications collected between 2013 and\n",
    "early 2020. We fine-tune the 5 base models4 on these news sources using the MLM objective for only 1 training epoch. We use the HuggingFace library (Wolf et al., 2020). '"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%reset -f\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import os\n",
    "import transformers\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, DatasetDict, dataset_dict\n",
    "#import datasets"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# try with the data downloaded from kaggle\n",
    "# https://www.kaggle.com/snapcrack/all-the-news/version/4\n",
    "allthenews = load_dataset('csv', script_version='master', data_files=['../data/external/archive/articles1.csv', '../data/external/archive/articles2.csv', '../data/external/archive/articles3.csv'], \n",
    "                          column_names = ['Unnamed', 'id', 'title', 'publication', 'author', 'date', 'year', 'month', 'url', 'content'])\n",
    "\n",
    "# get ArrowInvalid error if I also load the other two csvs and dont specify the column names"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default-1fabe3ad6ed75c39\n",
      "Reusing dataset csv (/home/alina/.cache/huggingface/datasets/csv/default-1fabe3ad6ed75c39/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Select only articles of one news source"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "allthebreitbart = allthenews.filter(lambda example: example['publication']=='Breitbart').remove_columns(['Unnamed', 'title', 'publication', 'author', 'year', 'month', 'url', 'date'])\n",
    "allthefox = allthenews.filter(lambda example: example['publication']=='Fox News').remove_columns(['Unnamed', 'title', 'publication', 'author', 'year', 'month', 'url', 'date'])\n",
    "allthereuters = allthenews.filter(lambda example: example['publication']=='Reuters').remove_columns(['Unnamed', 'title', 'publication', 'author', 'year', 'month', 'url', 'date'])\n",
    "alltheguardian = allthenews.filter(lambda example: example['publication']=='Guardian').remove_columns(['Unnamed', 'title', 'publication', 'author', 'year', 'month', 'url', 'date'])\n",
    "#allthenewyorker = allthenews.filter(lambda example: example['publication']=='New Yorker').remove_columns(['Unnamed', 'title', 'publication', 'author', 'year', 'month', 'url', 'date'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loading cached processed dataset at /home/alina/.cache/huggingface/datasets/csv/default-1fabe3ad6ed75c39/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-fdf8e1d56953b4fa.arrow\n",
      "Loading cached processed dataset at /home/alina/.cache/huggingface/datasets/csv/default-1fabe3ad6ed75c39/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-9bdb7c6a06f3e03f.arrow\n",
      "Loading cached processed dataset at /home/alina/.cache/huggingface/datasets/csv/default-1fabe3ad6ed75c39/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-c4684eb61f540615.arrow\n",
      "Loading cached processed dataset at /home/alina/.cache/huggingface/datasets/csv/default-1fabe3ad6ed75c39/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-27a90a56f71a1670.arrow\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "allthenews_dict = {'breitbart': allthebreitbart, 'fox': allthefox, 'reuters': allthereuters, 'guardian': alltheguardian}#, 'newyorker': allthenewyorker}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def train_val_split(dat, validation_percentage=0.1):\n",
    "    # simple function to split existing huggingface Dataset object. \n",
    "    # returns DatasetDict containing training and validation dataset\n",
    "    # default split ratio 90-10\n",
    "    # note that validation set is still named test as I am using datasets train_test_split method\n",
    "    train_valid = dat.train_test_split(test_size=0.1)\n",
    "    return train_valid\n",
    "\n",
    "# adapted from https://discuss.huggingface.co/t/how-to-split-main-dataset-into-train-dev-test-as-datasetdict/1090/2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "newspapers = ['breitbart', 'fox', 'reuters', 'guardian']#, 'newyorker']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "allthenews_trainval = {newspaper : train_val_split(allthenews_dict[newspaper]['train']) for newspaper, dat in allthenews_dict.items()}"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loading cached split indices for dataset at /home/alina/.cache/huggingface/datasets/csv/default-1fabe3ad6ed75c39/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-c99456e9f66ae51d.arrow and /home/alina/.cache/huggingface/datasets/csv/default-1fabe3ad6ed75c39/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-1580c791c6a4aea0.arrow\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Identity terms"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# gender terms\n",
    "female_terms = [\"girls\", \"women\", \"females\", \"girlfriends\", \"stepmothers\", \"ladies\", \"sisters\", \"mothers\", \"grandmothers\" \"wives\", \"brides\", \"schoolgirls\", \"mommies\"]\n",
    "male_terms = [\"men\", \"males\", \"boys\" \"boyfriends\", \"stepfathers\", \"gentlemen\" \"brothers\", \"fathers\", \"grandfathers\", \"husbands\", \"grooms\",  \"schoolboys\",  \"daddies\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# deciding to add some synonyms, singulars, plurals etc\n",
    "female_terms = [\"she\", \"her\", \"girl\", \"girls\", \"woman\", \"women\", \"female\", \"females\", \"girlfriend\", \"girlfriends\", \"stepmothers\", \"lady\", \"ladies\", \"sister\", \"sisters\", \"mother\", \"mothers\", \"grandmothers\", \"wife\", \"wives\", \"bride\", \"brides\", \"schoolgirls\", \"mom\", \"mum\", \"moms\", \"mums\", \"mummies\", \"mommies\", \"miss\", \"mrs\", \"ms\", \"lady\", \"mistress\"]\n",
    "male_terms = [\"he\", \"his\", \"him\", \"boy\", \"boys\", \"man\", \"men\", \"male\", \"males\", \"boyfriend\", \"boyfriends\", \"stepfathers\", \"gentleman\", \"gentlemen\", \"brother\", \"brothers\", \"father\", \"fathers\", \"grandfathers\", \"husband\", \"husbands\", \"groom\", \"grooms\",  \"schoolboys\", \"dad\", \"dads\", \"daddy\", \"daddies\", \"mr\", \"sir\", \"lord\"]\n",
    "neutral_terms = [\"they\", \"their\", \"them\", \"child\", \"person\", \"people\", \"parent\", \"parents\", \"partner\", \"partners\", \"spouse\", \"sibling\", \"siblings\"]\n",
    "female_terms_short = [\"she\", \"woman\", \"girl\"]#her\", \"girl\", \"girls\", \"woman\", \"women\", \"female\"]\n",
    "male_terms_short = [\"he\", \"man\", \"boy\"]#his\", \"him\", \"boy\", \"boys\", \"man\", \"men\", \"male\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "race_terms = list(map(lambda x: x.lower(), [\"Asians\", \"Americans\", \"Europeans\", \"Jews\", \"Indians\", \"Russians\", \"Africans\", \"Black people\", \"Mexicans\", \"Whites\", \"Blacks\", \"White people\", \"Germans\", \"blondes\", \"blonde girls\", \"Arabs\", \"White Americans\", \"Black Americans\", \"Hispanics\", \"Native Americans\", \"Black men\", \"White men\", \"Asian women\", \"Asian men\", \"Black women\", \"the Dutch\", \"Irish people\", \"Irish men\", \"White women\", \"Indian men\", \"Indian women\", \"Chinese men\", \"Chinese women\", \"Japanese women\", \"Japanese men\", \"Indian parents\", \"Asian parents\", \"White parents\", \"Black parents\", \"Black fathers\", \"Latinas\", \"Latinos\", \"Latin people\", \"Brazilian women\",\"Asian kids\", \"Black kids\", \"White kids\", \"African Americans\", \"Nigerians\", \"Ethiopians\", \"Ukrainians\", \"Sudanese people\", \"Afghans\", \"Iraqis\", \"Hispanic men\", \"Hispanic women\", \"Italians\", \"Italian men\", \"Italian women\", \"Somalis\", \"Iranian people\", \"Iranians\", \"Australians\", \"Australian men\", \"Australian women\", \"Aussies\", \"Ghanaians\", \"Swedes\", \"Finns\", \"Venezuelans\", \"Moroccans\", \"Syrians\", \"Pakistanis\", \"British people\", \"French people\", \"Greeks\", \"Indonesians\", \"Vietnamese people\", \"Romanians\", \"Ecuadorians\", \"Norwegians\", \"Nepalis\" , \"Scots\", \"Bengalis\", \"Polish people\", \"Taiwanese people\", \"Albanians\", \"Colombians\", \"Egyptians\", \"Koreans\", \"Persian people\", \"Portuguese men\", \"Portuguese women\", \"Turkish people\", \"Austrians\", \"South Africans\", \"Dutch people\", \"Chileans\", \"Lebanese people\"]))\n",
    "race_terms_short = list(map(lambda x: x.lower(), [\"Asian\", \"American\", \"European\", \"Jewish\", \"Indian\", \"African\", \"Black\", \"Mexican\", \"White\", \"Arab\"]))\n",
    "social_gps_paper = list(map(lambda x: x.lower(), ['christian', 'police', 'conservative', 'celebrities', 'gay', 'academics', 'Iraq', 'asian', 'black', 'ladies', 'teenager']))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "def load_news(input_dir: str, newspaper: str):\n",
    "    # load entire dataset, discard columns we don't need\n",
    "    news = load_dataset('csv', script_version='master', data_files=[input_dir+'/articles1.csv', input_dir+'/articles2.csv', input_dir+'/articles3.csv'], \n",
    "                          column_names = ['Unnamed', 'id', 'title', 'publication', 'author', 'date', 'year', 'month', 'url', 'content']).remove_columns(['Unnamed', 'title', 'author', 'year', 'month', 'url', 'date'])\n",
    "    \n",
    "    # filter for publication of interest\n",
    "    news_newspaper = news.filter(lambda example: example['publication']==newspaper).remove_columns(['publication'])\n",
    "    \n",
    "    return news_newspaper\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import Emotion Lexicon"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "emotionwords = pd.read_excel('../data/external/NRC-Emotion-Lexicon-v0.92-In105Languages-Nov2017Translations.xlsx', usecols=\"A,DB:DK\")\n",
    "emotions = list(emotionwords.columns[1:])\n",
    "emotionwords_dict = {emotion: list(emotionwords['English (en)'].loc[emotionwords[emotion]==1]) for emotion in emotions}\n",
    "# cast series to list. change back if this breaks things\n",
    "del emotionwords"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Co-occurrences: How often is eg. woman mentioned close to eg. angry?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "# option 1. text module: search concordances for words in emotion lexicon with re matching then Counter()\n",
    "# 1b find concordances, take those as input to construct a vocab and extract counts\n",
    "# option 2. lm module: count ngrams that contain a specific word\n",
    "# option 3: do this: take bigram collocation finder, start with window size 2 (adjacent words) \n",
    "# and count the occurrence of emotion words among the results\n",
    "# then vary the window size"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Taking a look at attention"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# fine concordances using only hugginface datasets package"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModel"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "dataset_test = allthenews_dict['breitbart'].filter(lambda example: 'girl' in example['content'])#['train'].remove_columns(['id'])\n",
    "model_checkpoint = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loading cached processed dataset at /home/alina/.cache/huggingface/datasets/csv/default-1fabe3ad6ed75c39/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-5a17f8f896ab12fa.arrow\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"content\"])#, return_tensors='pt')\n",
    "\n",
    "tokenized_test = dataset_test.map(tokenize_function, batched=True, num_proc=4, remove_columns=['id', 'content'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " #0:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[AToken indices sequence length is longer than the specified maximum sequence length for this model (1418 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (634 > 512). Running this sequence through the model will result in indexing errors\n",
      " #0: 100%|██████████| 1/1 [00:01<00:00,  1.06s/ba]\n",
      "\n",
      "\n",
      " #3: 100%|██████████| 1/1 [00:01<00:00,  1.11s/ba]\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1583 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2644 > 512). Running this sequence through the model will result in indexing errors\n",
      " #1: 100%|██████████| 1/1 [00:01<00:00,  1.23s/ba]\n",
      "\n",
      " #2: 100%|██████████| 1/1 [00:01<00:00,  1.25s/ba]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "tokenized_test"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'token_type_ids'],\n",
       "        num_rows: 1165\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "# block_size = tokenizer.model_max_length # 512\n",
    "block_size = 128\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "snippets_test = tokenized_test.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " #0:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "\u001b[A\n",
      "\n",
      " #1: 100%|██████████| 1/1 [00:02<00:00,  2.10s/ba]\n",
      " #0: 100%|██████████| 1/1 [00:02<00:00,  2.74s/ba]\n",
      "\n",
      "\n",
      " #3: 100%|██████████| 1/1 [00:02<00:00,  3.00s/ba]\n",
      "\n",
      " #2: 100%|██████████| 1/1 [00:03<00:00,  3.09s/ba]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "girl_snippets = snippets_test['train'].filter(lambda example: 2611 in example['input_ids'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 11/11 [00:01<00:00,  7.35ba/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "girl_snippets"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'labels', 'token_type_ids'],\n",
       "    num_rows: 960\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "source": [
    "import torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "source": [
    "girl_snippets_ids = torch.tensor(girl_snippets['input_ids'])\n",
    "girl_snippets_token_type_ids = torch.tensor(girl_snippets['token_type_ids'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "# # Load model and retrieve attention\n",
    "# model_version = 'bert-base-uncased'\n",
    "# do_lower_case = True\n",
    "# model = BertModel.from_pretrained(model_version, output_attentions=True)\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)\n",
    "# sentence_a = \"The cat sat on the mat\"\n",
    "# sentence_b = \"The cat lay on the rug\"\n",
    "# inputs = tokenizer.encode_plus(sentence_a, sentence_b, return_tensors='pt', add_special_tokens=True)\n",
    "# token_type_ids = inputs['token_type_ids']\n",
    "# input_ids = inputs['input_ids']\n",
    "# attention = model(input_ids, token_type_ids=token_type_ids)[-1]\n",
    "\n",
    "\n",
    "# #input_id_list = input_ids[0].tolist() # Batch index 0\n",
    "# #tokens = tokenizer.convert_ids_to_tokens(input_id_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "del tokenized_news\n",
    "del allthenews_dict, allthenews, allthenews_trainval, allthebreitbart, allthefox, alltheguardian"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "# use bert-base first\n",
    "model_version = 'bert-base-uncased'\n",
    "do_lower_case = True\n",
    "model_bert_base = BertModel.from_pretrained(model_version, output_attentions=True)\n",
    "tokenizer_bert_base = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# def concordance_to_attention(concordances, model, tokenizer):\n",
    "#     tokenized_concordances = tokenizer(concordances, padding=True, return_tensors='pt')\n",
    "#     token_type_ids = tokenized_concordances['token_type_ids']\n",
    "#     input_ids = tokenized_concordances['input_ids']\n",
    "#     attentions = model(input_ids, token_type_ids=token_type_ids)[-1]\n",
    "#     return attentions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "def group_tokens_to_ids(tokens, tokenizer):\n",
    "    ''' return dict {token : id} if tokens is a list of tokens \n",
    "    or a dict {category : {token : id} } if tokens is a dict of type {category : token}\n",
    "    tokens that are not recognised and are mapped to the id 100 for the unknown token are left out'''\n",
    "    if type(tokens) == list:\n",
    "        ids = {token: idx for token, idx in zip(tokens, tokenizer.convert_tokens_to_ids(tokens))}\n",
    "    if type(tokens) == dict:\n",
    "        ids = {cat: {token: idx for token, idx in zip(tokens[cat], tokenizer.convert_tokens_to_ids(tokens[cat])) if idx != 100} for cat in tokens.keys()}\n",
    "    return ids"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "# female_terms_ids = group_tokens_to_ids(['girl', 'woman', 'she'], tokenizer_bert_base)\n",
    "# emotion_terms_ids = group_tokens_to_ids(emotionwords_dict, tokenizer_bert_base)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# def attention_weighted_counts(text_snippets, tokenizer, attentions, identity_word, emotion_tokens, layer_id, attention_head_nr):\n",
    "#     '''\n",
    "#     text_snippet -- list of concordances centered arouned identity word\n",
    "#     attention -- attention output by passing text_snippet to some model\n",
    "#     identity_word -- some word that comes up in every text snippet that is used as point of reference. ie use attention from this word to the other words for the weighting\n",
    "#     '''\n",
    "#     # get id of identity_word that serves as reference point\n",
    "#     ref_wd_id = tokenizer.convert_tokens_to_ids(identity_word)\n",
    "#     # number of times identity word occurrs. should be length of text_snippets\n",
    "#     N = len(text_snippets)\n",
    "#     # get list of lists with tuples (word_idx, attention from reference word to this word)\n",
    "#     weighted_cts_test = [[(idx.item(), attention.item()/N) for idx, attention in zip(text_snippets[sentence_id], attentions[layer_id][sentence_id, attention_head_nr, text_snippets[sentence_id].tolist().index(ref_wd_id),:])] for sentence_id in range(text_snippets.shape[0])]\n",
    "#     # flatten list\n",
    "#     weighted_cts_test = [item for sublist in weighted_cts_test for item in sublist]\n",
    "#     # emotion tokens to ids\n",
    "#     emotion_ids = group_tokens_to_ids(emotion_tokens, tokenizer)\n",
    "#     cts = {emotion: sum([sum([ct[1] for ct in weighted_cts_test if ct[0] == emotion_term_id]) for emotion_term, emotion_term_id in emotion_ids[emotion].items()]) for emotion in emotions}\n",
    "#     return cts"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "def attention_weighted_counts(newspaper: str, reference_word: str, model_path, layer_id: int, attention_head_id: int):\n",
    "    '''\n",
    "    newspaper : str\n",
    "    reference_word : str\n",
    "    model_path : str\n",
    "    layer_id : int\n",
    "    attention_head_id : int\n",
    "\n",
    "    Example usage:\n",
    "    attention_weighted_counts('Breitbart', 'girl', 'bert-base-uncased', 0, 11)\n",
    "\n",
    "    '''\n",
    "\n",
    "    model = AutoModel.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    #tokenized_concordances = tokenizer(concordances, padding=True, return_tensors='pt')\n",
    "    #token_type_ids = tokenized_concordances['token_type_ids']\n",
    "    #input_ids = tokenized_concordances['input_ids']\n",
    "\n",
    "    # load emotion words from file\n",
    "    emotionwords = pd.read_excel('../data/external/NRC-Emotion-Lexicon-v0.92-In105Languages-Nov2017Translations.xlsx', usecols=\"A,DB:DK\")\n",
    "    emotions = list(emotionwords.columns[1:])\n",
    "    # check if this works\n",
    "    emotion_tokens = {emotion: list(emotionwords['English (en)'].loc[emotionwords[emotion]==1]) for emotion in emotions}\n",
    "\n",
    "\n",
    "    # load news paper corpora\n",
    "    # load entire dataset, discard columns we don't need\n",
    "    news = load_dataset('csv', script_version='master', data_files=[input_dir+'/articles1.csv', input_dir+'/articles2.csv', input_dir+'/articles3.csv'], \n",
    "                          column_names = ['Unnamed', 'id', 'title', 'publication', 'author', 'date', 'year', 'month', 'url', 'content']).remove_columns(['Unnamed', 'title', 'author', 'year', 'month', 'url', 'date'])\n",
    "    \n",
    "    # filter for publication of interest\n",
    "    news_newspaper = news.filter(lambda example: example['publication']==newspaper).remove_columns(['publication'])\n",
    "    \n",
    "    # find articles that contain the word\n",
    "    articles_w_ref_wd = news_newspaper.filter(lambda example: reference_word in example['content'])\n",
    "\n",
    "    # tokenize\n",
    "    articles_tokenized = articles_w_ref_wd.map(tokenize_function, batched=True, num_proc=4, remove_columns=['id', 'content'])\n",
    "\n",
    "    # split data into processable chunks\n",
    "    snippets = articles_tokenized.map(\n",
    "            group_texts,\n",
    "            batched=True,\n",
    "            batch_size=1000,\n",
    "            num_proc=4,\n",
    "        )\n",
    "\n",
    "    # find idx of reference word\n",
    "    idx = tokenizer.convert_ids_to_tokens(reference_word) # check\n",
    "\n",
    "    # discard snippets that don't have the reference word\n",
    "    snippets_wd = snippets['train'].filter(lambda example: idx in example['input_ids'])\n",
    "\n",
    "    # cast to tensor\n",
    "    input_ids = torch.tensor(snippets_wd['input_ids'])\n",
    "    token_type_ids = torch.tensor(snippets_wd['token_type_ids'])\n",
    "    \n",
    "    # pass the chunks to the model to get attention\n",
    "    attentions = model(input_ids, token_type_ids=token_type_ids)[-1]\n",
    "    print('--calculated attentions--')\n",
    "    #cts = attention_weighted_counts(tokenized_concordances['input_ids'], tokenizer, attentions, reference_word, emotion_tokens, layer_id, attention_head_id)\n",
    "    #print(attentions.shape)\n",
    "    \n",
    "    # get id of identity_word that serves as reference point\n",
    "    ref_wd_id = tokenizer.convert_tokens_to_ids(reference_word)\n",
    "    print('Reference word id: ', ref_wd_id)\n",
    "    # number of times identity word occurrs. should be length of text_snippets\n",
    "    N = len(input_ids)\n",
    "\n",
    "    # get list of lists with tuples (word_idx, attention from reference word to this word)\n",
    "\n",
    "    w_cts = [[(idx.item(), attention.item()/N) for idx, attention in zip(input_ids[sentence_id], attentions[layer_id][sentence_id, attention_head_id, input_ids[sentence_id].tolist().index(ref_wd_id),:])] for sentence_id in range(input_ids.shape[0])]\n",
    "    print('--collected counts--')\n",
    "    # flatten list\n",
    "    w_cts = [item for sublist in w_cts for item in sublist]\n",
    "    \n",
    "    # emotion tokens to ids\n",
    "    emotion_ids = group_tokens_to_ids(emotion_tokens, tokenizer)\n",
    "    \n",
    "    # sum over words in each emotion category\n",
    "#     sum_w_cts = {}\n",
    "#     for emotion in emotions.keys():\n",
    "#             for token, idx in emotion_ids[emotion]:\n",
    "#                 sum([ct[1] for ct in w_cts if ct[0] == emotion_term_id])\n",
    "                \n",
    "                \n",
    "#         sum([sum([ct[1] for ct in w_cts if ct[0] == emotion_term_id]) for emotion_term, emotion_term_id in emotion_ids[emotion].items()])\n",
    "        \n",
    "        \n",
    "    sum_w_cts = {emotion: sum([sum([ct[1] for ct in w_cts if ct[0] == emotion_term_id]) for emotion_term, emotion_term_id in emotion_ids[emotion].items()]) for emotion in emotions}\n",
    "    \n",
    "    return sum_w_cts"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "cts = attention_weighted_counts(girl_snippets_ids, girl_snippets_token_type_ids, 'girl', 'bert-base-uncased', emotionwords_dict, 11, 0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'torch.Tensor'>\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "#  For each layer add all attention heads\n",
    "#  Then have a dataframe with rows the emotion categories and columns layers 0, ..., 12, total\n",
    "# w_cts_fox_girl_0_0 = attention_weighted_counts(concordances_fox_girl, 'girl', model_bert_base, tokenizer_bert_base, emotionwords_dict, 0, 0)\n",
    "# w_cts_fox_boy_0_0 = attention_weighted_counts(concordances_fox_boy, 'boy', model_bert_base, tokenizer_bert_base, emotionwords_dict, 0, 0)\n",
    "# w_cts_fox_woman = attention_weighted_counts(concordances_fox_woman, 'woman', model_bert_base, tokenizer_bert_base, emotionwords_dict, 0, 0)\n",
    "# w_cts_fox_man = attention_weighted_counts(concordances_fox_man, 'man', model_bert_base, tokenizer_bert_base, emotionwords_dict, 0, 0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "w_cts_fox_girl_11_0 = attention_weighted_counts(concordances_fox_girl, 'girl', model_bert_base, tokenizer_bert_base, emotionwords_dict, 11, 0)\n",
    "w_cts_fox_boy_11_0 = attention_weighted_counts(concordances_fox_boy, 'boy', model_bert_base, tokenizer_bert_base, emotionwords_dict, 11, 0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--calculated attentions--\n",
      "--collected counts--\n",
      "--calculated attentions--\n",
      "--collected counts--\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "round(pd.DataFrame([w_cts_fox_girl_11_0, w_cts_fox_boy_11_0], index=['girl', 'boy']).T*10e3, 1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>girl</th>\n",
       "      <th>boy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Positive</th>\n",
       "      <td>19.0</td>\n",
       "      <td>19.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Negative</th>\n",
       "      <td>16.5</td>\n",
       "      <td>50.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anger</th>\n",
       "      <td>8.3</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anticipation</th>\n",
       "      <td>11.0</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Disgust</th>\n",
       "      <td>2.9</td>\n",
       "      <td>34.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fear</th>\n",
       "      <td>11.8</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Joy</th>\n",
       "      <td>11.3</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sadness</th>\n",
       "      <td>9.9</td>\n",
       "      <td>9.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Surprise</th>\n",
       "      <td>6.6</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trust</th>\n",
       "      <td>14.4</td>\n",
       "      <td>17.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              girl   boy\n",
       "Positive      19.0  19.5\n",
       "Negative      16.5  50.3\n",
       "Anger          8.3  10.4\n",
       "Anticipation  11.0   9.5\n",
       "Disgust        2.9  34.8\n",
       "Fear          11.8  14.0\n",
       "Joy           11.3   5.8\n",
       "Sadness        9.9   9.7\n",
       "Surprise       6.6   3.9\n",
       "Trust         14.4  17.5"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "# should I delete stuff in between so it stops crashing\n",
    "w_cts_guardian_girl = attention_weighted_counts(concordances_guardian_girl, 'girl', model_bert_base, tokenizer_bert_base, emotionwords_dict, 0, 0)\n",
    "w_cts_guardian_boy = attention_weighted_counts(concordances_guardian_boy, 'boy', model_bert_base, tokenizer_bert_base, emotionwords_dict, 0, 0)\n",
    "w_cts_guardian_woman = pd.DataFrame.from_dict(attention_weighted_counts(concordances_guardian_woman, 'woman', model_bert_base, tokenizer_bert_base, emotionwords_dict, 0, 0))\n",
    "# w_cts_guardian_man = pd.DataFrame.from_dict(attention_weighted_counts(concordances_guardian_man, 'man', model_bert_base, tokenizer_bert_base, emotionwords_dict, 0, 0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--calculated attentions--\n",
      "--collected counts--\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3514/3675627714.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# should I delete stuff in between so it stops crashing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#w_cts_guardian_girl = attention_weighted_counts(concordances_guardian_girl, 'girl', model_bert_base, tokenizer_bert_base, emotionwords_dict, 0, 0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mw_cts_guardian_boy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_weighted_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcordances_guardian_boy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'boy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_bert_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_bert_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memotionwords_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mw_cts_guardian_woman\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_weighted_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcordances_guardian_woman\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'woman'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_bert_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_bert_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memotionwords_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# w_cts_guardian_man = pd.DataFrame.from_dict(attention_weighted_counts(concordances_guardian_man, 'man', model_bert_base, tokenizer_bert_base, emotionwords_dict, 0, 0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3514/892010525.py\u001b[0m in \u001b[0;36mattention_weighted_counts\u001b[0;34m(concordances, reference_word, model, tokenizer, emotion_tokens, layer_id, attention_head_id)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0msum_w_cts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0memotion\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mct\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw_cts\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0memotion_term_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0memotion_term\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memotion_term_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0memotion_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0memotion\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0memotion\u001b[0m \u001b[0;32min\u001b[0m \u001b[0memotions\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msum_w_cts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3514/892010525.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0msum_w_cts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0memotion\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mct\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw_cts\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0memotion_term_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0memotion_term\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memotion_term_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0memotion_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0memotion\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0memotion\u001b[0m \u001b[0;32min\u001b[0m \u001b[0memotions\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msum_w_cts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3514/892010525.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0msum_w_cts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0memotion\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mct\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw_cts\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0memotion_term_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0memotion_term\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memotion_term_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0memotion_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0memotion\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0memotion\u001b[0m \u001b[0;32min\u001b[0m \u001b[0memotions\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msum_w_cts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3514/892010525.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0msum_w_cts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0memotion\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mct\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw_cts\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0memotion_term_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0memotion_term\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memotion_term_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0memotion_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0memotion\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0memotion\u001b[0m \u001b[0;32min\u001b[0m \u001b[0memotions\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msum_w_cts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "pd.DataFrame([w_cts_guardian_girl, w_cts_guardian_boy], index=['girl', 'boy']).T.round(4)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>girl</th>\n",
       "      <th>boy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Positive</th>\n",
       "      <td>0.0219</td>\n",
       "      <td>0.0213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Negative</th>\n",
       "      <td>0.0156</td>\n",
       "      <td>0.0293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anger</th>\n",
       "      <td>0.0071</td>\n",
       "      <td>0.0094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anticipation</th>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Disgust</th>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fear</th>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Joy</th>\n",
       "      <td>0.0119</td>\n",
       "      <td>0.0102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sadness</th>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Surprise</th>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.0077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trust</th>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                girl     boy\n",
       "Positive      0.0219  0.0213\n",
       "Negative      0.0156  0.0293\n",
       "Anger         0.0071  0.0094\n",
       "Anticipation  0.0105  0.0100\n",
       "Disgust       0.0049  0.0183\n",
       "Fear          0.0096  0.0100\n",
       "Joy           0.0119  0.0102\n",
       "Sadness       0.0079  0.0096\n",
       "Surprise      0.0059  0.0077\n",
       "Trust         0.0117  0.0129"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "w_cts_guardian_girl_11_0 = attention_weighted_counts(concordances_guardian_girl, 'girl', model_bert_base, tokenizer_bert_base, emotionwords_dict, 11, 0)\n",
    "#w_cts_guardian_boy_11_0 = attention_weighted_counts(concordances_guardian_boy, 'boy', model_bert_base, tokenizer_bert_base, emotionwords_dict, 11, 0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# round(pd.DataFrame([w_cts_guardian_girl_11_0, w_cts_guardian_boy_11_0], index=['girl', 'boy']).T*10e3, 2)\n",
    "round(pd.DataFrame([w_cts_guardian_girl_11_0], index=['girl']).T*10e3, 2)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "Error",
     "evalue": "Session cannot generate requests",
     "traceback": [
      "Error: Session cannot generate requests",
      "at w.executeCodeCell (/home/alina/.vscode/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:301310)",
      "at w.execute (/home/alina/.vscode/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:300703)",
      "at w.start (/home/alina/.vscode/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:296367)",
      "at runMicrotasks (<anonymous>)",
      "at processTicksAndRejections (internal/process/task_queues.js:93:5)",
      "at async t.CellExecutionQueue.executeQueuedCells (/home/alina/.vscode/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:311160)",
      "at async t.CellExecutionQueue.start (/home/alina/.vscode/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:310700)"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Fox, girl: ', w_cts_fox_girl.round(4))\n",
    "print('Fox, boy: ', w_cts_fox_boy.round(4))\n",
    "print('Fox, man: ', w_cts_fox_man.round(4))\n",
    "print('Fox, woman: ', w_cts_fox_woman.round(4))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Guardian, girl: ', w_cts_guardian_girl.round(4))\n",
    "print('Guardian, boy: ', w_cts_guardian_boy.round(4))\n",
    "print('Guardian, man: ', w_cts_guardian_man.round(4))\n",
    "print('Guardian, woman: ', w_cts_guardian_woman.round(4))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load fine tuned models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from transformers import BertModel, BertTokenizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "tokenizer_bert_guardian = BertTokenizer.from_pretrained('../../stereotypes_in_lms/bert_guardian/')\n",
    "model_bert_guardian = BertModel.from_pretrained('../../stereotypes_in_lms/bert_guardian/')\n",
    "tokenizer_bert_breitbart = BertTokenizer.from_pretrained('../../stereotypes_in_lms/bert_breitbart/')\n",
    "model_bert_breitbart = BertModel.from_pretrained('../../stereotypes_in_lms/bert_breitbart/')\n",
    "tokenizer_bert_reuters = BertTokenizer.from_pretrained('../../stereotypes_in_lms/bert_reuters/')\n",
    "model_bert_reuters = BertModel.from_pretrained('../../stereotypes_in_lms/bert_reuters/')\n",
    "tokenizer_bert_fox = BertTokenizer.from_pretrained('../../stereotypes_in_lms/bert_fox/')\n",
    "model_bert_fox = BertModel.from_pretrained('../../stereotypes_in_lms/bert_fox/')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at ../../stereotypes_in_lms/bert_guardian/ were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ../../stereotypes_in_lms/bert_guardian/ and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at ../../stereotypes_in_lms/bert_breitbart/ were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ../../stereotypes_in_lms/bert_breitbart/ and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at ../../stereotypes_in_lms/bert_reuters/ were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ../../stereotypes_in_lms/bert_reuters/ and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at ../../stereotypes_in_lms/bert_fox/ were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ../../stereotypes_in_lms/bert_fox/ and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "attention_weighted_counts(concordances_fox_girl, 'girl', model_bert_fox, tokenizer_bert_fox, emotionwords_dict, 0, 0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# visualise weights on some examples\n",
    "# https://colab.research.google.com/drive/1YoJqS9cPGu3HL2_XExw3kCsRBtySQS2v?usp=sharing"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('py38': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "interpreter": {
   "hash": "a93a4bc11815328d32ede22d7d72c6abb9033c5be012e744f9f698902f5ecd01"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}